{"cells":[{"metadata":{"_uuid":"7a7eb966545f4514ec61ec7964894cd0a9e646d1"},"cell_type":"markdown","source":"# Building a Recommender System with Podcasts\n\nI listen to a lot of podcasts. If you're looking for some to start listedning too, my favorites are The MFCEO Project, The School of Greatness with Lewis Howes, and UMD Newman Catholic Campus Ministry.\n\nI mainly use Apple's Podcast app to find and listen to podcasts (although I just started using Spotify - it functions *much* better), but I had no way of finding new podcasts aside from browsing the 'featured' page. When I build the courage to drudge through the \"new and noteworthy' section, I  get exhausted looking through all the podcasts that simply don't interest me. There isn't a great way for me to find new podcasts that I like.\n\nEnter: Recommender Systems.\n\nRecommender systems take the things that you like, and find other similar things. All your favorite apps and businesses use one - Spotify, Netflix, Amazon. You'd be hard pressed to find any content consumption platform that serve you recommendation in some shape or form (unless that platform is Apple Podcasts). PS: if you're an analyst and want to provide *immense* value to your company, build a recommender system if they don't already have one. It's a simple project, and can catapult your analytics compitencies forward.\n\nIn this kernel, I'm going to use iTunes podcast data to build a recommender system, and see if I can't find some new podcasts that I'd like. A quick thank you to Chris Clark, whose tutorial I used to build this recommender system. Go check out his blog for a lot of useful posts!\n\nFirst, let's import some packages. We're going to import a few of the usuals: NumPy, Pandas, and OS to read , manipulate the data. We're also going to import some not-so-usual packages, both from the sklearn module: TfidfVectorizer and linear_kernel. I'll explain what each do a little bit later when I'm ready to use them."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nprint(os.listdir(\"../input\"))\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ce24207f8761f28e496d86c9b5e4ce2c348f9e15"},"cell_type":"markdown","source":"This data provided by ListenNotes has both podcast and episode metadata. I'm only going to be working with the podcast metadata in this example, although you could theoretically build a recommender system for individual episodes."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"podcasts = pd.read_csv('../input/podcasts.csv')\npodcasts.head()\npodcasts.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aec67b5c2c4bd315539bd3eb5ea8d37844c92cdd"},"cell_type":"markdown","source":"This dataset has over 120,000 podcasts! I'm not going to be able to use all that data so I'm going to take a sample. But first we need to clean this up a bit. \n\n\"Language\" is a column, so I expect there might be some podcasts that aren't in English, which could throw a wrench in the recommendations."},{"metadata":{"trusted":true,"_uuid":"08597f38dca90112331fc39b9e8afea212effe03"},"cell_type":"code","source":"(podcasts\n .language\n .value_counts()\n .to_frame()\n .head(10))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"32b265f8ff3a2330eb62fb2f1860bbb99b02d826"},"cell_type":"markdown","source":"It looks like there are podcasts in many different langauges in this data. I'm only going to include those that are recorded in English."},{"metadata":{"trusted":true,"_uuid":"80cf1ed055e31b586c9a7b084912fcb179feaedc"},"cell_type":"code","source":"podcasts = podcasts[podcasts.language == 'English']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a09d3b06591dd52a8fd6ad2869802c6b100a114"},"cell_type":"markdown","source":"When I loaded the data, I also noticed that there were some records that didn't have a description. These aren't going to be very useful, so I'm going to drop those records.\n\nJust to be sure we're working with cleaned data, I'm also going to drop any records that might be duplicates."},{"metadata":{"trusted":true,"_uuid":"fa4b24d01588a0ec78099eaa27a32a924b10d9b5"},"cell_type":"code","source":"podcasts = podcasts.dropna(subset=['description'])\npodcasts = podcasts.drop_duplicates('itunes_id')\nsum(podcasts.description.isnull())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6f429ec5564a52dbfd86d4bc044fb54190ff28d7"},"cell_type":"markdown","source":"Since we're building a recommender system based on podcast descriptions, we want to make sure that the descriptions have enough sontent in them to serve as useful inputs. Below I'm going to find the length of each podcast description and describe it."},{"metadata":{"trusted":true,"_uuid":"d35cfdcb86921f16397b85f1c1bc077877167cf8"},"cell_type":"code","source":"podcasts['description_length'] = [len(x.description.split()) for _, x in podcasts.iterrows()]\npodcasts['description_length'].describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1c2cfa1853a915fcbd5e23aa85a14b504fa11c4a"},"cell_type":"markdown","source":"At lease a quarter of our descriptions have less than 11 words. I'm certain these won't serve as good inputs when we build the recommender system. Just to be safe, I'm only going to include descriptions that have at least 20 words in them."},{"metadata":{"trusted":true,"_uuid":"65541b97868e15a8dc7c76df97ad19f8f3e5cff0"},"cell_type":"code","source":"podcasts = podcasts[podcasts.description_length >= 20]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8fe656e53775d9b8f6d6becff00c402cedb1b2f1"},"cell_type":"markdown","source":"Like I mentioned earlier, I'm not going to use the entire dataset - I just don't have enough computaitonal power. Instead, I'm going to sample 15,000 records from this data set and build the recommender system on that sample.\n\nAt the end of this, I want to be able to find podcasts similar to the ones that I mentioned above, so I'm going to pull those into a seperate dataframe, and load them back in after I've created my sample."},{"metadata":{"trusted":true,"_uuid":"d69e8917295c11e5c0b9c5e559aa844eecdfad4d"},"cell_type":"code","source":"favorite_podcasts = ['The MFCEO Project', 'Up and Vanished', 'Lore']\nfavorites = podcasts[podcasts.title.isin(favorite_podcasts)]\nfavorites","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d69e8917295c11e5c0b9c5e559aa844eecdfad4d"},"cell_type":"code","source":"podcasts = podcasts[~podcasts.isin(favorites)].sample(15000)\ndata = pd.concat([podcasts, favorites], sort = True).reset_index(drop = True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6277fa06a92fe2b7e0340ea8405c8d6dd770456"},"cell_type":"markdown","source":"Here comes the fun! Now that I have my dataset prepared, I can start to build the recommender system.\n\nI'm going to use TfidVecortizer to find the **term frequency inverse document frequency** (tf-idf or TFIDF). This measure is meant to score how *important* a word in it's document. TFIDF uses two measures to find the most important words: **term frequency** and **inverse-document frequency**.\n\nLet's assume the example below is one ad out of 100 ads found in a magazine.\n\nCleaning the gutters in your home is crutial in keeping the exterior of your hom ein wonderful condition. Out pattented gutter guard blocked debries from clogging your gutter, and saves your the trouble of cleaning them yourself.\n\nIn the example above, the word 'your' appears four times. This is the raw term frequency which is used in scikit-learn's TfidfVectorizer function. There area a few other ways to calcualte term frequency that involves normalization - you can read more about those here.\n\n'Your' is a pretty common word, so let's assume it's in 90 out of the 100 ads. The formula for inverse-document frequncy is:\n\nlog(Number of total documents / Number of documents containing the term) + 1\n\nIn this case, the inverse-document frequency would be log(100/90), or 1.04. This is a relatively low score, and shows us that 'your' doesn't reflect what the ad above is talking about. If we used a word like \"gutter\", we could expect a different result.\n\nTfidfVectorizer allows you to remove stopwords, include bigrams, and filter out words based on how frequently they appear. You can read more about all the parameters here..\n\nIn this example, I'm removing English stop words, and looking at bigrams and trigrams (collections of terms in groups of two and three)."},{"metadata":{"trusted":true,"_uuid":"6896dcd5b69d09cce6757905ec87672965de0c0d"},"cell_type":"code","source":"tf = TfidfVectorizer(analyzer = 'word', ngram_range = (1, 3), min_df = 0, stop_words = \"english\")\ntf_idf = tf.fit_transform(data['description'])\ntf_idf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84292dd5860908b1759ceb4e67791cb3a1314af8"},"cell_type":"markdown","source":"What's returned is a sparse matrix that contains the tf-idf values of each word, bigram, and trigram in the podcast descriptions. \n\nNext, we're going to use the linear_kernal function to calculate the similarity between the podcasts. If two podcasts have tfidf scores that are close to each other, this value is going to be close to 1. If they don't share any similar scores, the score will be closer to 0. This is the score we're going to use to find similar podcasts."},{"metadata":{"trusted":true,"_uuid":"5a2cc881e493aa8667b5e14bd86f9a948deb6612","scrolled":true},"cell_type":"code","source":"similarity = linear_kernel(tf_idf, tf_idf)\nsimilarity","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"23bbc1fc2139d3c5f488f850cff9d1b8734cc4a9"},"cell_type":"markdown","source":"We can now use this similarity dataframe to find similar podcasts. While we could return scores for every podcast, I'm only going examine the 3 most similar podcasts.\n\nLet's take a look at podcasts similar to \"Up and Vanished\"."},{"metadata":{"trusted":true,"_uuid":"4e9477f961f07e820f5b0eab06ef6c130014a12d","scrolled":true},"cell_type":"code","source":"x = data[data.title == 'Up and Vanished'].index[0]\nsimilar_idx = similarity[x].argsort(axis = 0)[-4:-1]\nfor i in similar_idx:\n    print(similarity[x][i], '-', data.title[i], '-', data.description[i], '\\n')\nprint('Original - ' + data.description[x])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a7c315c8dd1151a1d40f8a7d05f9337a7f6644fc"},"cell_type":"markdown","source":"These look like some good recommendations, but there's something interesting about the most similar podcast \"VANISHED: The Tara Calico Investication\" an our original  podcast \"Up and Vanished\". They're both investigating the disappearnce of a woman names Tara.\n\nThis highlights where content recommendation systems fails. When there are unique words that don't help in describing the topic of the content (names, uncommon adjectives, company name, etc.), the recommendations get skewed. Look at the recommendations for another one of my favorites: Lore."},{"metadata":{"trusted":true,"_uuid":"620014582b2c5ba27b29e62528ee633a8c785d5a"},"cell_type":"code","source":"x = data[data.title == 'Lore'].index[0]\nsimilar_idx = similarity[x].argsort(axis = 0)[-4:-1]\nfor i in similar_idx:\n    print(similarity[x][i], '-', data.title[i], '-', data.description[i], '\\n')\nprint('Original - ' + data.description[x])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d6d60993132462ccae83ac27b07cb522858a9c61"},"cell_type":"markdown","source":"Here, we don't get three horror podcasts, rather we get podcasts that contain the word \"bi-weekly\". \n\nContent based recommendations are great for building a quick data product, but fall short in offering good recommendations, especially when the content is limited. This method would work much better if we were recommending articles, or books. If you were to use collabortive filtering to build a recommender system with this dataset, it should be paired with a collaborative recommender system to make it more effective.\n\nThanks for reading through my kernel! I have a few other ideas on what to do with these podcast descriptions, so stay tuned for more!+"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}